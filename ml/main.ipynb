{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from surprise import SVD, Dataset, Reader\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Connect to Elasticsearch\n",
    "\n",
    "es = Elasticsearch([\"http://localhost:9200\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahya\\AppData\\Local\\Temp\\ipykernel_21944\\1787596816.py:11: DeprecationWarning: Received 'size' via a specific parameter in the presence of a 'body' parameter, which is deprecated and will be removed in a future version. Instead, use only 'body' or only specific parameters.\n",
      "  resp = es.search(index=index_name, body=query, size=batch_size, scroll=\"2m\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating\n",
      "0   16140     3705     2.0\n",
      "1   16140     3717     5.0\n",
      "2   16140     3745     4.0\n",
      "3   16140     3751     3.0\n",
      "4   16140     3753     4.0\n"
     ]
    }
   ],
   "source": [
    "# 2) Pull Ratings Data\n",
    "\n",
    "def fetch_all_ratings(es, index_name=\"ratings\", batch_size=10000):\n",
    "    \"\"\"\n",
    "    Example: a simple scroll to fetch all docs from 'ratings' index\n",
    "    \"\"\"\n",
    "    ratings = []\n",
    "    query = {\n",
    "        \"query\": {\"match_all\": {}}\n",
    "    }\n",
    "    resp = es.search(index=index_name, body=query, size=batch_size, scroll=\"2m\")\n",
    "    \n",
    "    scroll_id = resp[\"_scroll_id\"]\n",
    "    hits = resp[\"hits\"][\"hits\"]\n",
    "    \n",
    "    while len(hits) > 0:\n",
    "        for h in hits:\n",
    "            source = h[\"_source\"]\n",
    "            # userId, movieId, rating\n",
    "            ratings.append({\n",
    "                \"userId\": source[\"userId\"],\n",
    "                \"movieId\": source[\"movieId\"],\n",
    "                \"rating\": source[\"rating\"]\n",
    "            })\n",
    "        resp = es.scroll(scroll_id=scroll_id, scroll=\"2m\")\n",
    "        scroll_id = resp[\"_scroll_id\"]\n",
    "        hits = resp[\"hits\"][\"hits\"]\n",
    "    \n",
    "    return pd.DataFrame(ratings)\n",
    "\n",
    "ratings_df = fetch_all_ratings(es, \"ratings\")\n",
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahya\\AppData\\Local\\Temp\\ipykernel_21944\\1900206303.py:6: DeprecationWarning: Received 'size' via a specific parameter in the presence of a 'body' parameter, which is deprecated and will be removed in a future version. Instead, use only 'body' or only specific parameters.\n",
      "  resp = es.search(index=index_name, body=query, size=batch_size, scroll=\"2m\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                               title               genres  \\\n",
      "0     3217              Star Is Born, A (1937)              [Drama]   \n",
      "1     3218                       Poison (1991)              [Drama]   \n",
      "2     3219              Pacific Heights (1990)  [Mystery, Thriller]   \n",
      "3     3220                   Night Tide (1961)              [Drama]   \n",
      "4     3221  Draughtsman's Contract, The (1982)              [Drama]   \n",
      "\n",
      "                                         description  popularity  vote_average  \n",
      "0  Esther Blodgett is just another starry-eyed fa...      13.408         7.200  \n",
      "1  A trio of interweaved transgressive tales, tel...       4.791         6.100  \n",
      "2  A couple works hard to renovate their dream ho...      13.862         6.200  \n",
      "3  A young sailor falls in love with a mysterious...       6.999         6.331  \n",
      "4  A young artist is commissioned by the wife of ...      13.244         7.100  \n"
     ]
    }
   ],
   "source": [
    "# 3) Pull Movies Data\n",
    "\n",
    "def fetch_all_movies(es, index_name=\"movies\", batch_size=10000):\n",
    "    movies = []\n",
    "    query = {\"query\": {\"match_all\": {}}}\n",
    "    resp = es.search(index=index_name, body=query, size=batch_size, scroll=\"2m\")\n",
    "    \n",
    "    scroll_id = resp[\"_scroll_id\"]\n",
    "    hits = resp[\"hits\"][\"hits\"]\n",
    "    \n",
    "    while len(hits) > 0:\n",
    "        for h in hits:\n",
    "            source = h[\"_source\"]\n",
    "            # Extract relevant fields\n",
    "            movies.append({\n",
    "                \"movieId\": int(source[\"movieId\"]) if \"movieId\" in source else None,\n",
    "                \"title\": source.get(\"title\",\"\"),\n",
    "                \"genres\": source.get(\"genres\", []),\n",
    "                \"description\": source.get(\"description\",\"\"),\n",
    "                \"popularity\": source.get(\"popularity\", 0.0),\n",
    "                \"vote_average\": source.get(\"vote_average\", 0.0)\n",
    "            })\n",
    "        resp = es.scroll(scroll_id=scroll_id, scroll=\"2m\")\n",
    "        scroll_id = resp[\"_scroll_id\"]\n",
    "        hits = resp[\"hits\"][\"hits\"]\n",
    "    \n",
    "    return pd.DataFrame(movies)\n",
    "\n",
    "movies_df = fetch_all_movies(es, \"movies\")\n",
    "print(movies_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.to_pickle('movies.pkl')\n",
    "ratings_df.to_pickle('ratings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 25         item: 247        r_ui = None   est = 3.47   {'was_impossible': False}\n",
      "Details for movieId 247:\n",
      "      movieId                      title          genres  \\\n",
      "9038      247  Heavenly Creatures (1994)  [Crime, Drama]   \n",
      "\n",
      "                                            description  popularity  \\\n",
      "9038  Precocious teenager Juliet moves to New Zealan...      13.149   \n",
      "\n",
      "      vote_average  \n",
      "9038         6.983  \n"
     ]
    }
   ],
   "source": [
    "# 4) Basic Collaborative Filtering\n",
    "\n",
    "reader = Reader(rating_scale=(0.5, 5.0))  # or (0,5) depending on dataset\n",
    "data = Dataset.load_from_df(ratings_df[[\"userId\",\"movieId\",\"rating\"]], reader) # Surprise requires (user, item, rating)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "algo = SVD(n_factors=50, reg_all=0.02)  # hyperparams\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Example prediction for user=25, movie=247\n",
    "pred = algo.predict(uid=25, iid=247)\n",
    "print(pred)  # gives a rating estimate\n",
    "\n",
    "# Check which movie corresponds to movieId = 247\n",
    "movie_details = movies_df[movies_df[\"movieId\"] == 247]\n",
    "if not movie_details.empty:\n",
    "    print(f\"Details for movieId 247:\\n{movie_details}\")\n",
    "else:\n",
    "    print(\"movieId 247 not found in movies_df.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD model saved as svd_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained SVD model to a file\n",
    "with open(\"svd_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(algo, f)\n",
    "\n",
    "print(\"SVD model saved as svd_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.7922  0.7926  0.7928  0.7930  0.7936  0.7928  0.0005  \n",
      "MAE (testset)     0.6000  0.6003  0.6003  0.6006  0.6011  0.6005  0.0004  \n",
      "Fit time          32.40   33.60   36.38   35.98   36.30   34.93   1.63    \n",
      "Test time         13.45   9.43    12.21   9.66    12.38   11.43   1.60    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.79218921, 0.79257247, 0.79281246, 0.79299006, 0.79362065]),\n",
       " 'test_mae': array([0.59996896, 0.60031046, 0.60030699, 0.6005947 , 0.60108422]),\n",
       " 'fit_time': (32.39742302894592,\n",
       "  33.60471248626709,\n",
       "  36.376347064971924,\n",
       "  35.98498201370239,\n",
       "  36.29843854904175),\n",
       " 'test_time': (13.454914808273315,\n",
       "  9.434318542480469,\n",
       "  12.20736813545227,\n",
       "  9.656645774841309,\n",
       "  12.384940385818481)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Perform cross-validation on the SVD model\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This value, \"est\", represents the system's estimate of how user 25 would rate movie 247, based on patterns learned from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Content Approach\n",
    "\n",
    "# a) For each movie, build a content vector (genres + optional text from 'description')\n",
    "# b) For each user, you can do user-based CF or combine user’s CF-based latent vector \n",
    "#    with content similarities.\n",
    "\n",
    "# Example: Just compute a simple TF-IDF on \"description\" + multi-hot encode \"genres\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Multi-hot genres\n",
    "def multi_hot_genres(df, all_genres):\n",
    "    # all_genres: union set of all possible genres\n",
    "    # returns a DataFrame with genre_ columns\n",
    "    out_df = df.copy()\n",
    "    for g in all_genres:\n",
    "        out_df[f\"genre_{g}\"] = out_df[\"genres\"].apply(lambda x: 1 if g in x else 0)\n",
    "    return out_df\n",
    "\n",
    "all_genre_set = set()\n",
    "for g_list in movies_df[\"genres\"]:\n",
    "    for g in g_list:\n",
    "        all_genre_set.add(g)\n",
    "\n",
    "movies_enriched = multi_hot_genres(movies_df, all_genre_set)\n",
    "\n",
    "# Simple TF-IDF on description\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=200)\n",
    "desc_matrix = tfidf.fit_transform(movies_enriched[\"description\"])\n",
    "\n",
    "# Combine TF-IDF matrix + genre multi-hot as a single vector\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "genre_cols = [c for c in movies_enriched.columns if c.startswith(\"genre_\")]\n",
    "genre_matrix = movies_enriched[genre_cols].values  # shape: (num_movies, num_genres)\n",
    "\n",
    "content_matrix = hstack([desc_matrix, genre_matrix])  # shape: (num_movies, 200 + num_genres)\n",
    "\n",
    "# Now 'content_matrix' is a feature vector for each movie\n",
    "# You can compute item-item similarity or incorporate this into a hybrid model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature dimensions: 220\n",
      "Reduced feature dimensions: 110\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Apply TruncatedSVD to reduce dimensions\n",
    "n_components = 110  # Adjust based on your dataset and performance\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "content_matrix_reduced = svd.fit_transform(content_matrix)\n",
    "\n",
    "print(f\"Original feature dimensions: {content_matrix.shape[1]}\")\n",
    "print(f\"Reduced feature dimensions: {content_matrix_reduced.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_matrix_reduced = content_matrix_reduced.astype(np.float32)\n",
    "np.save(\"content_matrix_reduced.npy\", content_matrix_reduced)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors indexed: 62423\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Define dimensions after TruncatedSVD\n",
    "dim = content_matrix_reduced.shape[1]\n",
    "\n",
    "# Initialize FAISS index\n",
    "# 'IndexFlatIP' uses inner product, suitable for cosine similarity when vectors are normalized\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "\n",
    "# Normalize vectors to unit length for cosine similarity\n",
    "faiss.normalize_L2(content_matrix_reduced)\n",
    "\n",
    "# Add vectors to the index\n",
    "index.add(content_matrix_reduced)\n",
    "\n",
    "print(f\"Number of vectors indexed: {index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Movie - MovieID: 3227, Title: Not Love, Just Frenzy (Más que amor, frenesí) (1996)\n",
      "MovieID: 160434, Title: Doc Martin (2001), Similarity Score: 0.9109\n",
      "MovieID: 104451, Title: Dealing: Or the Berkeley-to-Boston Forty-Brick Lost-Bag Blues (1972), Similarity Score: 0.9109\n",
      "MovieID: 54988, Title: Stories of Lost Souls (2005), Similarity Score: 0.9109\n",
      "MovieID: 109, Title: Headless Body in Topless Bar (1995), Similarity Score: 0.9010\n",
      "MovieID: 204438, Title: Gone with the Woman (2007), Similarity Score: 0.8849\n"
     ]
    }
   ],
   "source": [
    "# Example: Find top-5 similar movies to the 10th movie in the DataFrame\n",
    "i = 10  # Movie index in the DataFrame\n",
    "k = 5   # Number of similar movies to retrieve\n",
    "\n",
    "# Retrieve the vector for movie i\n",
    "query_vector = content_matrix_reduced[i].reshape(1, -1)\n",
    "\n",
    "# Perform the search\n",
    "distances, indices = index.search(query_vector, k + 1)  # k+1 because the first result is the movie itself\n",
    "\n",
    "# Print the details of the movie at index i\n",
    "movie_id = movies_df.iloc[i][\"movieId\"]\n",
    "title = movies_df.iloc[i][\"title\"]\n",
    "print(f\"Query Movie - MovieID: {movie_id}, Title: {title}\")\n",
    "\n",
    "# Exclude the first result (the movie itself)\n",
    "similar_movie_indices = indices[0][1:]\n",
    "similar_distances = distances[0][1:]\n",
    "\n",
    "# Map indices back to movie IDs and titles\n",
    "for idx, dist in zip(similar_movie_indices, similar_distances):\n",
    "    movie_id = movies_df.iloc[idx][\"movieId\"]\n",
    "    title = movies_df.iloc[idx][\"title\"]\n",
    "    print(f\"MovieID: {movie_id}, Title: {title}, Similarity Score: {dist:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved as faiss_index.index\n"
     ]
    }
   ],
   "source": [
    "# Save the FAISS index to a file\n",
    "faiss.write_index(index, \"faiss_index.index\")\n",
    "print(\"FAISS index saved as faiss_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarities for Retrieved Movies: [0.91086346 0.91086346 0.91086346 0.9010469  0.8849232 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_vector = content_matrix_reduced[i].reshape(1, -1)\n",
    "similar_vectors = content_matrix_reduced[similar_movie_indices]\n",
    "cosine_similarities = cosine_similarity(query_vector, similar_vectors)\n",
    "\n",
    "print(\"Cosine Similarities for Retrieved Movies:\", cosine_similarities[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queried the system to find similar movies to the 10th movie in the DataFrame.\n",
    "#  system identified the movies listed above as the closest matches to the query movie\n",
    "# The similarity score ranges from 0 to 1 (closer to 1 indicates higher similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieID: 5748, Title: Inquisitor, The (a.k.a. Under Suspicion) (Garde à vue) (1981), Combined Score: 2.3995\n",
      "MovieID: 1221, Title: Godfather: Part II, The (1974), Combined Score: 2.4019\n",
      "MovieID: 87042, Title: Wanda (1970), Combined Score: 2.4069\n",
      "MovieID: 130912, Title: In the Shadow (2012), Combined Score: 2.4074\n",
      "MovieID: 1804, Title: Newton Boys, The (1998), Combined Score: 2.4095\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD, Dataset, Reader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Collaborative Filtering (CF) - Already Built\n",
    "# ratings_df = fetch_all_ratings(es, \"ratings\")\n",
    "# CF model: 'algo'\n",
    "\n",
    "# 2. Content-Based Filtering (CBF) - FAISS Index Built\n",
    "# movies_df = fetch_all_movies(es, \"movies\")\n",
    "# FAISS index: 'index'\n",
    "\n",
    "# 3. Hybrid Recommendation Function\n",
    "def get_hybrid_recommendations(user_id, movie_id, cf_algo, faiss_index, movies_df, content_matrix_reduced, top_k=5, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Combine CF and CBF to generate hybrid recommendations.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: ID of the user\n",
    "    - movie_id: ID of the movie to find recommendations similar to\n",
    "    - cf_algo: Trained CF algorithm (e.g., SVD)\n",
    "    - faiss_index: FAISS index for CBF\n",
    "    - movies_df: DataFrame containing movie metadata\n",
    "    - content_matrix_reduced: Numpy array of reduced content features\n",
    "    - top_k: Number of recommendations to return\n",
    "    - alpha: Weight for CF scores (1 - alpha for CBF)\n",
    "    \n",
    "    Returns:\n",
    "    - List of recommended movie titles with combined scores\n",
    "    \"\"\"\n",
    "    # 1. Collaborative Filtering Score\n",
    "    cf_pred = cf_algo.predict(uid=user_id, iid=movie_id)\n",
    "    cf_score = cf_pred.est  # Estimated rating\n",
    "    \n",
    "    # 2. Content-Based Similarity\n",
    "    # Find the index of the movie_id in movies_df\n",
    "    try:\n",
    "        movie_idx = movies_df[movies_df[\"movieId\"] == movie_id].index[0]\n",
    "    except IndexError:\n",
    "        print(f\"MovieID {movie_id} not found in movies_df.\")\n",
    "        return []\n",
    "    \n",
    "    query_vector = content_matrix_reduced[movie_idx].reshape(1, -1)\n",
    "    faiss.normalize_L2(query_vector)\n",
    "    \n",
    "    distances, indices = faiss_index.search(query_vector, top_k + 1)\n",
    "    \n",
    "    similar_movie_indices = indices[0][1:]  # Exclude itself\n",
    "    similar_distances = distances[0][1:]\n",
    "    \n",
    "    # Compute similarity scores (higher is better)\n",
    "    similarity_scores = 1 - similar_distances  # Since 'angular' distance correlates with cosine similarity\n",
    "    \n",
    "    # 3. Combine CF and CBF Scores\n",
    "    combined_scores = alpha * cf_score + (1 - alpha) * similarity_scores\n",
    "    \n",
    "    recommendations = []\n",
    "    for idx, score in zip(similar_movie_indices, combined_scores):\n",
    "        recommended_movie_id = movies_df.iloc[idx][\"movieId\"]\n",
    "        recommended_title = movies_df.iloc[idx][\"title\"]\n",
    "        recommendations.append({\n",
    "            \"movieId\": recommended_movie_id,\n",
    "            \"title\": recommended_title,\n",
    "            \"combined_score\": score\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example Usage\n",
    "user_id = 25\n",
    "movie_id = 247  # Replace with a valid movie ID from your dataset\n",
    "recommendations = get_hybrid_recommendations(user_id, movie_id, algo, index, movies_df, content_matrix_reduced, top_k=5, alpha=0.7)\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"MovieID: {rec['movieId']}, Title: {rec['title']}, Combined Score: {rec['combined_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
